{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Santiago Valencia García - A00395902"
      ],
      "metadata": {
        "id": "5yksrQ13Ve7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZ33rowtaSaU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define constants\n",
        "INPUT_SIZE = 1     # Size of input data (1 for binary input)\n",
        "MEMORY_SIZE = 128  # Number of memory locations\n",
        "MEMORY_DIM = 20    # Dimensionality of each memory slot\n",
        "CONTROLLER_HIDDEN_SIZE = 100  # Size of controller hidden layer\n",
        "SEQ_LEN = 10       # Length of the input sequence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Command Unit\n",
        "\n",
        "The controller serves as the primary processing unit for the NTM, producing signals that manage memory operations. Here, it consists of an LSTM layer followed by a fully connected (fc) layer. The LSTM layer interprets patterns in the input sequence, while the fc layer condenses the output for efficient use by the rest of the NTM. Through control vectors created from current inputs and previous memory outputs, the controller directs the NTM’s actions at each time step."
      ],
      "metadata": {
        "id": "oqmuj-IOXP5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Controller network\n",
        "class Controller(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Controller, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, _ = self.rnn(x)\n",
        "        return self.fc(h[:, -1, :])"
      ],
      "metadata": {
        "id": "bz509eZ4Ww7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memory\n",
        "\n",
        "The memory in an NTM provides extended data retention, supporting the model’s capacity to store information independently from the controller’s short-term memory. Structured as a `memory_size x memory_dim` tensor, each row represents an individual memory cell. The `Memory` class offers read and write methods: reading retrieves data through address-based weights, while writing uses erase and add vectors to update specific cells. This structure is differentiable, enabling the NTM to learn effective memory access and update processes throughout training."
      ],
      "metadata": {
        "id": "I7TswLwSXk_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory module\n",
        "class Memory(nn.Module):\n",
        "    def __init__(self, memory_size, memory_dim):\n",
        "        super(Memory, self).__init__()\n",
        "        self.memory = torch.randn(memory_size, memory_dim) * 0.01\n",
        "\n",
        "    def read(self, address):\n",
        "        return torch.matmul(address.unsqueeze(0), self.memory).squeeze(0)\n",
        "\n",
        "    def write(self, address, erase_vector, add_vector):\n",
        "        address = address.view(-1, 1)\n",
        "        erase_matrix = address * erase_vector.unsqueeze(0)\n",
        "        add_matrix = address * add_vector.unsqueeze(0)\n",
        "        self.memory = self.memory * (1 - erase_matrix) + add_matrix"
      ],
      "metadata": {
        "id": "FmJIn8cJR6_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory Access Unit\n",
        "\n",
        "The Memory Access Unit bridges the Controller and the Memory, guiding which memory locations to access and modify. It creates an address vector to target specific memory locations, alongside erase and add vectors that control how those locations are altered. Here, linear layers convert the control vector into these crucial elements—address, erase, and add. In its `forward` function, the Memory Access Unit applies the memory updates and extracts the relevant data from memory, empowering the NTM to execute advanced memory operations."
      ],
      "metadata": {
        "id": "Ays-bI8ZX2xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read-Write head\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, memory_size, memory_dim):\n",
        "        super(Head, self).__init__()\n",
        "        self.memory_size = memory_size\n",
        "        self.memory_dim = memory_dim\n",
        "        self.addressing = nn.Linear(CONTROLLER_HIDDEN_SIZE, memory_size)\n",
        "        self.erase = nn.Linear(CONTROLLER_HIDDEN_SIZE, memory_dim)\n",
        "        self.add = nn.Linear(CONTROLLER_HIDDEN_SIZE, memory_dim)\n",
        "\n",
        "    def forward(self, control_vector, memory):\n",
        "        address_weights = torch.softmax(self.addressing(control_vector), dim=-1)\n",
        "        erase_vector = torch.sigmoid(self.erase(control_vector))\n",
        "        add_vector = torch.tanh(self.add(control_vector))\n",
        "        memory.write(address_weights, erase_vector, add_vector)\n",
        "        read_data = memory.read(address_weights)\n",
        "        return read_data"
      ],
      "metadata": {
        "id": "OPVcFYjOW07d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Output Module\n",
        "\n",
        "The Final Output Module generates the NTM’s ultimate output by combining processed data from the Controller and the Memory’s retrieved information. This layer, represented by a linear transformation within the `NTM` class, accepts the merged outputs of the Controller and Memory read vectors. It then produces predictions aligned with the target sequence for tasks like copying. The Final Output Module transforms the Controller's refined processing and Memory’s contextual insights into specific, task-related outputs."
      ],
      "metadata": {
        "id": "tXY7QGXJYALN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NTM Model Implementation\n",
        "class NeuralTuringMachine(nn.Module):\n",
        "    def __init__(self, input_dim, mem_size, mem_feature_dim, hidden_units):\n",
        "        super(NeuralTuringMachine, self).__init__()\n",
        "        # Initialize the components of the NTM: controller, memory, head, and output layer\n",
        "        self.controller_net = Controller(input_dim + mem_feature_dim, hidden_units, hidden_units)\n",
        "        self.memory_module = Memory(mem_size, mem_feature_dim)\n",
        "        self.memory_head = Head(mem_size, mem_feature_dim)\n",
        "        self.output_layer = nn.Linear(hidden_units + mem_feature_dim, input_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch, seq_length, _ = inputs.size()\n",
        "        memory_state = torch.zeros(batch, MEM_FEATURE_DIM).detach()  # Detach to prevent gradient flow\n",
        "        results = []\n",
        "\n",
        "        # Process each time step in the sequence\n",
        "        for timestep in range(seq_length):\n",
        "            controller_input = torch.cat([inputs[:, timestep, :], memory_state], dim=-1)\n",
        "            control_vector = self.controller_net(controller_input.unsqueeze(1))\n",
        "            memory_state = self.memory_head(control_vector, self.memory_module)\n",
        "            memory_state = memory_state.detach()  # Detach to avoid retaining unnecessary computation graph\n",
        "            result = self.output_layer(torch.cat([control_vector, memory_state], dim=-1))\n",
        "            results.append(result)\n",
        "\n",
        "        # Stack the results for each timestep into a single tensor\n",
        "        return torch.stack(results, dim=1)\n"
      ],
      "metadata": {
        "id": "a0wO0OllW32k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Process\n",
        "\n",
        "The train_neural_turing_machine() function manages the training of the NTM, running through multiple epochs to learn the copy task. It initializes the model, uses MSE loss, and applies the Adam optimizer. For each epoch, the model processes input sequences, computes the loss, and updates the weights using backpropagation. Progress is logged every 10 epochs to track learning."
      ],
      "metadata": {
        "id": "tOemPeRYYQuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate input and target sequences for the copying task\n",
        "def generate_copy_task_data(seq_len, batch_size=1):\n",
        "    # Create a random binary sequence\n",
        "    input_seq = torch.randint(0, 2, (batch_size, seq_len, INPUT_SIZE)).float()\n",
        "    # The target is the same as the input\n",
        "    target_seq = input_seq.clone()\n",
        "    return input_seq, target_seq"
      ],
      "metadata": {
        "id": "auHBSwcYW9aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for Neural Turing Machine\n",
        "def train_ntm():\n",
        "    # Initialize NTM model, loss function and optimizer\n",
        "    ntm = NTM(INPUT_SIZE, MEMORY_SIZE, MEMORY_DIM, CONTROLLER_HIDDEN_SIZE)\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error loss\n",
        "    optimizer = optim.Adam(ntm.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "    epochs = 100  # Total number of training epochs\n",
        "    for epoch in range(epochs):\n",
        "        # Generate input and target sequences for the current task\n",
        "        input_seq, target_seq = generate_copy_task_data(SEQ_LEN)\n",
        "\n",
        "        # Reset gradients before each epoch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform a forward pass through the model\n",
        "        output_seq = ntm(input_seq)\n",
        "\n",
        "        # Calculate the loss for the current prediction\n",
        "        loss = criterion(output_seq, target_seq)\n",
        "\n",
        "        # Backpropagate the error to adjust model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters using the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training progress every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "            print(\"Input Sequence:\")\n",
        "            print(input_seq.squeeze().numpy())\n",
        "            print(\"Expected Output Sequence:\")\n",
        "            print(target_seq.squeeze().numpy())\n",
        "            print(\"Model Output Sequence:\")\n",
        "            print(output_seq.detach().squeeze().numpy())\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "    return ntm, input_seq, target_seq\n",
        "\n",
        "\n",
        "# Execute the training process and evaluate the model's performance\n",
        "ntm, input_seq, target_seq = train_ntm()\n",
        "\n",
        "# Get the model's output for the final input sequence\n",
        "output_seq = ntm(input_seq).detach()\n",
        "\n",
        "# Function to display the final results\n",
        "def display_final_results(input_seq, target_seq, output_seq):\n",
        "    # Print the final input, expected output, and model output\n",
        "    print(\"\\nFinal Input Sequence:\")\n",
        "    print(input_seq.squeeze().numpy())\n",
        "    print(\"\\nFinal Expected Output Sequence:\")\n",
        "    print(target_seq.squeeze().numpy())\n",
        "    print(\"\\nFinal Model Output Sequence:\")\n",
        "    print(output_seq.detach().squeeze().numpy())\n",
        "\n",
        "    # Print the rounded and absolute values of the output for clarity\n",
        "    print(torch.absolute(torch.round(output_seq)).detach().squeeze().numpy())\n",
        "\n",
        "# Display the results\n",
        "display_final_results(input_seq, target_seq, output_seq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeJ_x6YQXBmQ",
        "outputId": "fdb5c1b4-5dd5-4110-f1a1-1a90c809965c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.42799314856529236\n",
            "Input sequence:\n",
            "[0. 1. 1. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Expected Output sequence:\n",
            "[0. 1. 1. 0. 0. 1. 1. 0. 0. 0.]\n",
            "NTM Output sequence:\n",
            "[-0.03352125 -0.03415407 -0.03394266 -0.03287514 -0.03267232 -0.0333279\n",
            " -0.03311973 -0.03205422 -0.03185463 -0.0316558 ]\n",
            "==================================================\n",
            "Epoch 10, Loss: 0.3283453583717346\n",
            "Input sequence:\n",
            "[0. 1. 1. 1. 0. 0. 0. 0. 1. 1.]\n",
            "Expected Output sequence:\n",
            "[0. 1. 1. 1. 0. 0. 0. 0. 1. 1.]\n",
            "NTM Output sequence:\n",
            "[0.1534404  0.2049212  0.20513059 0.20533809 0.16183257 0.16204457\n",
            " 0.1622567  0.16246797 0.20639877 0.20660222]\n",
            "==================================================\n",
            "Epoch 20, Loss: 0.20362956821918488\n",
            "Input sequence:\n",
            "[1. 0. 1. 0. 1. 0. 0. 1. 1. 1.]\n",
            "Expected Output sequence:\n",
            "[1. 0. 1. 0. 1. 0. 0. 1. 1. 1.]\n",
            "NTM Output sequence:\n",
            "[0.48428524 0.41273454 0.53281295 0.4132245  0.5333064  0.41370544\n",
            " 0.41394863 0.5340569  0.53429496 0.53451824]\n",
            "==================================================\n",
            "Epoch 30, Loss: 0.2358575165271759\n",
            "Input sequence:\n",
            "[0. 0. 1. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Expected Output sequence:\n",
            "[0. 0. 1. 0. 0. 1. 1. 0. 0. 0.]\n",
            "NTM Output sequence:\n",
            "[0.49293163 0.57658213 0.80357283 0.57700056 0.57717234 0.8041739\n",
            " 0.80436885 0.5777118  0.57788044 0.5780911 ]\n",
            "==================================================\n",
            "Epoch 40, Loss: 0.11785750091075897\n",
            "Input sequence:\n",
            "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
            "Expected Output sequence:\n",
            "[0. 1. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
            "NTM Output sequence:\n",
            "[0.38068366 0.70533323 0.41361758 0.7054813  0.7055218  0.705541\n",
            " 0.4137659  0.41386226 0.70578825 0.70582736]\n",
            "==================================================\n",
            "Epoch 50, Loss: 0.08272701501846313\n",
            "Input sequence:\n",
            "[1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Expected Output sequence:\n",
            "[1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "NTM Output sequence:\n",
            "[0.67945    0.27935663 0.27946627 0.2795557  0.2796448  0.27973348\n",
            " 0.27982184 0.6856777  0.27983382 0.2799412 ]\n",
            "==================================================\n",
            "Epoch 60, Loss: 0.038747549057006836\n",
            "Input sequence:\n",
            "[1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Expected Output sequence:\n",
            "[1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
            "NTM Output sequence:\n",
            "[0.83003414 0.20941037 0.8216542  0.8215083  0.20925799 0.8215472\n",
            " 0.20923162 0.20938939 0.20948812 0.20958644]\n",
            "==================================================\n",
            "Epoch 70, Loss: 0.005890882574021816\n",
            "Input sequence:\n",
            "[1. 1. 0. 1. 1. 0. 0. 0. 1. 0.]\n",
            "Expected Output sequence:\n",
            "[1. 1. 0. 1. 1. 0. 0. 0. 1. 0.]\n",
            "NTM Output sequence:\n",
            "[1.0124921  0.9764945  0.10634362 0.9765847  0.9763087  0.10610356\n",
            " 0.10639487 0.10654473 0.9767651  0.10635918]\n",
            "==================================================\n",
            "Epoch 80, Loss: 0.003937850706279278\n",
            "Input sequence:\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Expected Output sequence:\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "NTM Output sequence:\n",
            "[ 0.13135897 -0.0564434  -0.05618227 -0.05596254 -0.05574374 -0.05552602\n",
            "  0.98912996 -0.05576872  0.9891493  -0.05579125]\n",
            "==================================================\n",
            "Epoch 90, Loss: 0.0026634535752236843\n",
            "Input sequence:\n",
            "[0. 0. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Expected Output sequence:\n",
            "[0. 0. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
            "NTM Output sequence:\n",
            "[ 0.11797643 -0.03834794  1.0360657  -0.03855506  1.0360597   1.0356481\n",
            " -0.03884641 -0.03842236 -0.03820125 -0.037981  ]\n",
            "==================================================\n",
            "Training complete.\n",
            "\n",
            "Final Input sequence:\n",
            "[1. 1. 0. 1. 0. 0. 1. 1. 1. 0.]\n",
            "\n",
            "Final Expected Output sequence:\n",
            "[1. 1. 0. 1. 0. 0. 1. 1. 1. 0.]\n",
            "\n",
            "Final NTM Output sequence:\n",
            "[ 1.015285    0.99938834 -0.01060912  0.9995284  -0.01063387 -0.01024598\n",
            "  0.99975693  0.99937177  0.99913037 -0.01092486]\n",
            "[1. 1. 0. 1. 0. 0. 1. 1. 1. 0.]\n"
          ]
        }
      ]
    }
  ]
}